{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwaJ0eGHCkw"
      },
      "source": [
        "# LoRA Easy Training Colab\n",
        "[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/jelosus1)\n",
        "\n",
        "### Colab powered by [Lora_Easy_Training_Scripts_Backend](https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Learn how to use the colab [here](https://civitai.com/articles/4409).\n",
        "\n",
        "If you feel something is missing, want something to be added or simply found a bug, open an [issue](https://github.com/Jelosus2/Lora_Easy_Training_Colab/issues).\n",
        "\n",
        "---\n",
        "\n",
        "Last Update: February 3, 2025. Check the [full changelog](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#changelog)\n",
        "\n",
        "Changes:\n",
        "- Fixed slow tagging.\n",
        "- Fixed path issues when setting up directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CSz_rmldHZvh",
        "cellView": "form",
        "outputId": "583eb03d-e512-436f-8319-d1f6d4247773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing trainer...\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2 python3-pip-whl python3-setuptools-whl\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2 python3-pip-whl python3-setuptools-whl\n",
            "  python3.10-venv\n",
            "0 upgraded, 6 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 3,991 kB of archives.\n",
            "After this operation, 8,189 kB of additional disk space will be used.\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../1-libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../2-aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Selecting previously unselected package python3-pip-whl.\n",
            "Preparing to unpack .../3-python3-pip-whl_22.0.2+dfsg-1ubuntu0.6_all.deb ...\n",
            "Unpacking python3-pip-whl (22.0.2+dfsg-1ubuntu0.6) ...\n",
            "Selecting previously unselected package python3-setuptools-whl.\n",
            "Preparing to unpack .../4-python3-setuptools-whl_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-setuptools-whl (68.1.2-2~jammy3) ...\n",
            "Selecting previously unselected package python3.10-venv.\n",
            "Preparing to unpack .../5-python3.10-venv_3.10.12-1~22.04.10_amd64.deb ...\n",
            "Unpacking python3.10-venv (3.10.12-1~22.04.10) ...\n",
            "Setting up python3-setuptools-whl (68.1.2-2~jammy3) ...\n",
            "Setting up python3-pip-whl (22.0.2+dfsg-1ubuntu0.6) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up python3.10-venv (3.10.12-1~22.04.10) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Cloning into '/content/trainer'...\n",
            "remote: Enumerating objects: 388, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 388 (delta 179), reused 145 (delta 145), pack-reused 174 (from 2)\u001b[K\n",
            "Receiving objects: 100% (388/388), 7.80 MiB | 12.03 MiB/s, done.\n",
            "Resolving deltas: 100% (221/221), done.\n",
            "Submodule 'LyCORIS' (https://github.com/67372a/LyCORIS) registered for path 'lycoris'\n",
            "Submodule 'sd_scripts' (https://github.com/kohya-ss/sd-scripts) registered for path 'sd_scripts'\n",
            "Cloning into '/content/trainer/lycoris'...\n",
            "Cloning into '/content/trainer/sd_scripts'...\n",
            "Submodule path 'lycoris': checked out 'ca9f47d238bb67266acd7354ce31a72eea37bdd2'\n",
            "Submodule path 'sd_scripts': checked out '5a18a03ffcc2a21c6e884a25d041076911a79a2a'\n",
            "creating venv and installing requirements\n",
            "Tesla T4\n",
            "\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Collecting torch==2.6.0\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (768.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.4/768.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.21.0\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.2.0\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy==1.13.1\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Collecting fsspec\n",
            "  Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 KB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.10.0\n",
            "  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
            "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0\n",
            "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n",
            "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.0.0 sympy-1.13.1 torch-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0 typing-extensions-4.12.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Collecting xformers==0.0.29.post3\n",
            "  Downloading https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl (43.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.6.0 in ./venv/lib/python3.10/site-packages (from xformers==0.0.29.post3) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from xformers==0.0.29.post3) (2.1.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (2.21.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (12.3.1.170)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (1.13.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (9.1.0.70)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (3.13.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (10.3.5.147)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (12.4.127)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (0.6.2)\n",
            "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (2024.6.1)\n",
            "Requirement already satisfied: triton==3.2.0 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (3.2.0)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.6.0->xformers==0.0.29.post3) (12.4.127)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.6.0->xformers==0.0.29.post3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch==2.6.0->xformers==0.0.29.post3) (2.1.5)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.29.post3\n",
            "Obtaining file:///content/trainer/sd_scripts (from -r requirements.txt (line 48))\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate==0.33.0\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.44.0\n",
            "  Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers[torch]==0.25.0\n",
            "  Downloading diffusers-0.25.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.1.1\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.8.1.78\n",
            "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.7.0\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.9.0\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.44.0\n",
            "  Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lion-pytorch==0.0.6\n",
            "  Downloading lion_pytorch-0.0.6-py3-none-any.whl (4.2 kB)\n",
            "Collecting schedulefree==1.4\n",
            "  Downloading schedulefree-1.4.tar.gz (22 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-optimizer==3.5.0\n",
            "  Downloading pytorch_optimizer-3.5.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 KB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prodigy-plus-schedule-free==1.9.0\n",
            "  Downloading prodigy_plus_schedule_free-1.9.0-py3-none-any.whl (22 kB)\n",
            "Collecting prodigyopt==1.1.2\n",
            "  Downloading prodigyopt-1.1.2-py3-none-any.whl (10 kB)\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors==0.4.4\n",
            "  Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.5/435.5 KB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting altair==4.2.2\n",
            "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 KB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting easygui==0.98.3\n",
            "  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting toml==0.10.2\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting voluptuous==0.13.1\n",
            "  Downloading voluptuous-0.13.1-py3-none-any.whl (29 kB)\n",
            "Collecting huggingface-hub==0.24.5\n",
            "  Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 KB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imagesize==1.4.1\n",
            "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Collecting numpy<=2.0\n",
            "  Downloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich==13.7.0\n",
            "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 KB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.2.0\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in ./venv/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Collecting numpy<=2.0\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 KB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.20,>=0.19\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (3.13.1)\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>=4.27\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in ./venv/lib/python3.10/site-packages (from diffusers[torch]==0.25.0->-r requirements.txt (line 3)) (11.0.0)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting wcwidth>=0.2.5\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in ./venv/lib/python3.10/site-packages (from pytorch-lightning==1.9.0->-r requirements.txt (line 8)) (4.12.2)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 KB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in ./venv/lib/python3.10/site-packages (from pytorch-lightning==1.9.0->-r requirements.txt (line 8)) (2024.6.1)\n",
            "Collecting lightning-utilities>=0.4.2\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Collecting pandas>=0.18\n",
            "  Downloading pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting toolz\n",
            "  Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from altair==4.2.2->-r requirements.txt (line 18)) (3.1.4)\n",
            "Collecting jsonschema>=3.0\n",
            "  Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting entrypoints\n",
            "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 15)) (68.1.2)\n",
            "Collecting grpcio>=1.48.2\n",
            "  Downloading grpcio-1.73.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf!=4.24.0,>=3.19.6\n",
            "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 KB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 KB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m134.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six>1.9\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.12.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting referencing>=0.28.4\n",
            "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Collecting attrs>=22.2.0\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonschema-specifications>=2023.03.6\n",
            "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Collecting rpds-py>=0.7.1\n",
            "  Downloading rpds_py-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.0/387.0 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting tzdata>=2022.7\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 KB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 KB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 KB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: triton==3.2.0 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 15)) (2.1.5)\n",
            "Collecting zipp>=3.20\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.7/157.7 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 KB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting propcache>=0.2.0\n",
            "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 KB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
            "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 KB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 KB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.6.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 KB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: schedulefree\n",
            "  Building wheel for schedulefree (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for schedulefree: filename=schedulefree-1.4-py3-none-any.whl size=39334 sha256=aaff00f84daab8fa8a11cec49511988ffa8d3196f85860b75778eea7ec2730fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/4f/8e/15e0ac6c13041e94078d9e4f88d14c2cd00ec198bc0fc78e91\n",
            "Successfully built schedulefree\n",
            "Installing collected packages: wcwidth, voluptuous, sentencepiece, pytz, library, easygui, zipp, werkzeug, urllib3, tzdata, tqdm, toolz, toml, tensorboard-data-server, six, schedulefree, safetensors, rpds-py, regex, pyyaml, pygments, psutil, protobuf, propcache, prodigyopt, packaging, numpy, multidict, mdurl, markdown, imagesize, idna, grpcio, ftfy, frozenlist, entrypoints, einops, charset_normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, absl-py, yarl, tensorboard, requests, referencing, python-dateutil, opencv-python, markdown-it-py, lightning-utilities, importlib-metadata, aiosignal, rich, pandas, jsonschema-specifications, huggingface-hub, aiohttp, torchmetrics, tokenizers, pytorch-optimizer, prodigy-plus-schedule-free, lion-pytorch, jsonschema, diffusers, bitsandbytes, accelerate, transformers, pytorch-lightning, altair\n",
            "  Running setup.py develop for library\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.2\n",
            "    Uninstalling numpy-2.1.2:\n",
            "      Successfully uninstalled numpy-2.1.2\n",
            "Successfully installed absl-py-2.3.0 accelerate-0.33.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 altair-4.2.2 async-timeout-5.0.1 attrs-25.3.0 bitsandbytes-0.44.0 certifi-2025.6.15 charset_normalizer-3.4.2 diffusers-0.25.0 easygui-0.98.3 einops-0.7.0 entrypoints-0.4 frozenlist-1.7.0 ftfy-6.1.1 grpcio-1.73.1 huggingface-hub-0.24.5 idna-3.10 imagesize-1.4.1 importlib-metadata-8.7.0 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 library-0.0.0 lightning-utilities-0.14.3 lion-pytorch-0.0.6 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.6.2 numpy-1.26.4 opencv-python-4.8.1.78 packaging-25.0 pandas-2.3.0 prodigy-plus-schedule-free-1.9.0 prodigyopt-1.1.2 propcache-0.3.2 protobuf-6.31.1 psutil-7.0.0 pygments-2.19.2 python-dateutil-2.9.0.post0 pytorch-lightning-1.9.0 pytorch-optimizer-3.5.0 pytz-2025.2 pyyaml-6.0.2 referencing-0.36.2 regex-2024.11.6 requests-2.32.4 rich-13.7.0 rpds-py-0.25.1 safetensors-0.4.4 schedulefree-1.4 sentencepiece-0.2.0 six-1.17.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tokenizers-0.19.1 toml-0.10.2 toolz-1.0.0 torchmetrics-1.7.3 tqdm-4.67.1 transformers-4.44.0 tzdata-2025.2 urllib3-2.5.0 voluptuous-0.13.1 wcwidth-0.2.13 werkzeug-3.1.3 yarl-1.20.1 zipp-3.23.0\n",
            "Processing /content/trainer/custom_scheduler\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Using legacy 'setup.py install' for LoraEasyCustomOptimizer, since package 'wheel' is not installed.\n",
            "Installing collected packages: LoraEasyCustomOptimizer\n",
            "  Running setup.py install for LoraEasyCustomOptimizer ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed LoraEasyCustomOptimizer-1.0.0\n",
            "Collecting starlette\n",
            "  Downloading starlette-0.47.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]\n",
            "  Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (2.32.4)\n",
            "Collecting dadaptation\n",
            "  Downloading dadaptation-3.2.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Collecting pycloudflared\n",
            "  Downloading pycloudflared-0.2.0-py3-none-any.whl (7.3 kB)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting came-pytorch\n",
            "  Downloading came_pytorch-0.1.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting pytorch_optimizer==3.1.2\n",
            "  Downloading pytorch_optimizer-3.1.2-py3-none-any.whl (187 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wheel\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.10 in ./venv/lib/python3.10/site-packages (from pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (2.6.0+cu124)\n",
            "Collecting anyio<5,>=3.6.2\n",
            "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.10/site-packages (from starlette->-r ../requirements.txt (line 1)) (4.12.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Collecting click>=7.0\n",
            "  Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]->-r ../requirements.txt (line 2)) (6.0.2)\n",
            "Collecting python-dotenv>=0.13\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Collecting watchfiles>=0.13\n",
            "  Downloading watchfiles-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 KB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4\n",
            "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.6.3\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 KB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop>=0.15.1\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 3)) (2025.6.15)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 3)) (3.4.2)\n",
            "Collecting sentry-sdk>=2.0.0\n",
            "  Downloading sentry_sdk-2.32.0-py2.py3-none-any.whl (356 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.1/356.1 KB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting platformdirs\n",
            "  Downloading platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
            "Collecting pydantic<3\n",
            "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 KB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitpython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 KB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in ./venv/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 5)) (7.0.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in ./venv/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 5)) (6.31.1)\n",
            "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 5)) (25.0)\n",
            "Collecting tomli\n",
            "  Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from pycloudflared->-r ../requirements.txt (line 7)) (4.67.1)\n",
            "Collecting exceptiongroup>=1.0.2\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspection>=0.4.0\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Collecting pydantic-core==2.33.2\n",
            "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting annotated-types>=0.6.0\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (3.13.1)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (3.1.4)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (2.21.5)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (3.3)\n",
            "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (12.4.5.8)\n",
            "Requirement already satisfied: triton==3.2.0 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (1.13.1)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.10/site-packages (from torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (10.3.5.147)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.10->pytorch_optimizer==3.1.2->-r ../requirements.txt (line 10)) (2.1.5)\n",
            "Building wheels for collected packages: dadaptation\n",
            "  Building wheel for dadaptation (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dadaptation: filename=dadaptation-3.2-py3-none-any.whl size=23290 sha256=51123983b3ae25089af08ef24e8a264094983fd70a777429b69a4eff3cdadb1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/03/6d/feba04df15ef39d9ac4e3504058ac2a88fb2ef9183ba92b111\n",
            "Successfully built dadaptation\n",
            "Installing collected packages: wheel, websockets, uvloop, typing-inspection, tomli, sniffio, smmap, setproctitle, sentry-sdk, scipy, python-dotenv, pyngrok, pydantic-core, platformdirs, httptools, h11, exceptiongroup, dadaptation, click, annotated-types, uvicorn, pydantic, pycloudflared, gitdb, anyio, watchfiles, starlette, gitpython, wandb, pytorch_optimizer, came-pytorch\n",
            "  Attempting uninstall: pytorch_optimizer\n",
            "    Found existing installation: pytorch_optimizer 3.5.0\n",
            "    Uninstalling pytorch_optimizer-3.5.0:\n",
            "      Successfully uninstalled pytorch_optimizer-3.5.0\n",
            "Successfully installed annotated-types-0.7.0 anyio-4.9.0 came-pytorch-0.1.3 click-8.2.1 dadaptation-3.2 exceptiongroup-1.3.0 gitdb-4.0.12 gitpython-3.1.44 h11-0.16.0 httptools-0.6.4 platformdirs-4.3.8 pycloudflared-0.2.0 pydantic-2.11.7 pydantic-core-2.33.2 pyngrok-7.2.11 python-dotenv-1.1.1 pytorch_optimizer-3.1.2 scipy-1.15.3 sentry-sdk-2.32.0 setproctitle-1.3.6 smmap-5.0.2 sniffio-1.3.1 starlette-0.47.1 tomli-2.2.1 typing-inspection-0.4.1 uvicorn-0.35.0 uvloop-0.21.0 wandb-0.20.1 watchfiles-1.1.0 websockets-15.0.1 wheel-0.45.1\n",
            "Processing /content/trainer/lycoris\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in ./venv/lib/python3.10/site-packages (from lycoris-lora==3.1.1.post1) (0.7.0)\n",
            "Requirement already satisfied: toml in ./venv/lib/python3.10/site-packages (from lycoris-lora==3.1.1.post1) (0.10.2)\n",
            "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (from lycoris-lora==3.1.1.post1) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from lycoris-lora==3.1.1.post1) (4.67.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (11.6.1.9)\n",
            "Requirement already satisfied: triton==3.2.0 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (3.2.0)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (12.4.127)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (3.13.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (9.1.0.70)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (2.21.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (12.4.127)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (3.3)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (12.4.127)\n",
            "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.1.1.post1) (2024.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch->lycoris-lora==3.1.1.post1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch->lycoris-lora==3.1.1.post1) (2.1.5)\n",
            "Building wheels for collected packages: lycoris-lora\n",
            "  Building wheel for lycoris-lora (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lycoris-lora: filename=lycoris_lora-3.1.1.post1-py3-none-any.whl size=75804 sha256=9a923fef286a7db34e0fce88a9bc9abbe9c601e6f2db3005a57448bf05d38947\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j5jxu2jv/wheels/c6/bf/03/c4c42a90096afd59ad1556905e7fdc0a7fe40ffa41a62913ae\n",
            "Successfully built lycoris-lora\n",
            "Installing collected packages: lycoris-lora\n",
            "Successfully installed lycoris-lora-3.1.1.post1\n",
            "completed installing\n",
            "Installation complete!\n",
            "Downloading tagger script that allows v3 taggers...\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "026b2b|\u001b[1;32mOK\u001b[0m  |   2.3MiB/s|//content/trainer/sd_scripts/finetune/tag_images_by_wd14_tagger.py\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Fixing sd_scripts logging issue on colab...\n",
            "Found existing installation: rich 13.7.0\n",
            "Uninstalling rich-13.7.0:\n",
            "  Would remove:\n",
            "    /content/trainer/sd_scripts/venv/lib/python3.10/site-packages/rich-13.7.0.dist-info/*\n",
            "    /content/trainer/sd_scripts/venv/lib/python3.10/site-packages/rich/*\n",
            "Proceed (Y/n)?   Successfully uninstalled rich-13.7.0\n",
            "Finished installation!\n"
          ]
        }
      ],
      "source": [
        "# @title ## 1. Install the trainer ![doro](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro.png)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "root_path = Path(\"/content\")\n",
        "trainer_dir = root_path.joinpath(\"trainer\")\n",
        "\n",
        "venv_pip = trainer_dir.joinpath(\"sd_scripts/venv/bin/pip\")\n",
        "venv_python = trainer_dir.joinpath(\"sd_scripts/venv/bin/python\")\n",
        "\n",
        "# @markdown Execute the cell to install the trainer\n",
        "\n",
        "installed_dependencies = False\n",
        "first_step_done = False\n",
        "\n",
        "def install_trainer():\n",
        "  global installed_dependencies, first_step_done\n",
        "\n",
        "  print(\"Installing trainer...\")\n",
        "  !apt -y update -qq\n",
        "  !apt install -y python3.10-venv aria2 -qq\n",
        "\n",
        "  installed_dependencies = True\n",
        "\n",
        "  !git clone https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n",
        "\n",
        "  !chmod 755 /content/trainer/colab_install.sh\n",
        "  os.chdir(trainer_dir)\n",
        "  !./colab_install.sh\n",
        "\n",
        "  os.chdir(root_path)\n",
        "\n",
        "  first_step_done = True\n",
        "  print(\"Installation complete!\")\n",
        "\n",
        "def download_custom_wd_tagger():\n",
        "  global wd_path\n",
        "\n",
        "  wd_path = trainer_dir.joinpath(\"sd_scripts/finetune/tag_images_by_wd14_tagger.py\")\n",
        "\n",
        "  print(\"Downloading tagger script that allows v3 taggers...\")\n",
        "  !rm \"{wd_path}\"\n",
        "  !aria2c \"https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/main/custom/tag_images_by_wd14_tagger.py\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{wd_path}\"\n",
        "\n",
        "def fix_scripts_logging():\n",
        "  print(\"Fixing sd_scripts logging issue on colab...\")\n",
        "  !yes | {venv_pip} uninstall rich\n",
        "\n",
        "def main():\n",
        "  install_trainer()\n",
        "  download_custom_wd_tagger()\n",
        "  fix_scripts_logging()\n",
        "  print(\"Finished installation!\")\n",
        "\n",
        "try:\n",
        "  main()\n",
        "except Exception as e:\n",
        "  print(f\"Error intalling the trainer!\\n{e}\")\n",
        "  first_step_done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "oS4dJqXoiyC5",
        "outputId": "67e84e3d-2e48-4f4b-83ae-843d15b256e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up directories...\n",
            "Mounted at /content/drive\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# @title ## 2. Setup the directories ![doro diamond](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_diamond.png)\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "if not globals().get(\"first_step_done\"):\n",
        "  root_path = Path(\"/content\")\n",
        "  trainer_dir = root_path.joinpath(\"trainer\")\n",
        "\n",
        "drive_dir = root_path.joinpath(\"drive/MyDrive\")\n",
        "pretrained_model_dir = root_path.joinpath(\"pretrained_model\")\n",
        "vae_dir = root_path.joinpath(\"vae\")\n",
        "tagger_models_dir = root_path.joinpath(\"tagger_models\")\n",
        "\n",
        "# @markdown The base path for your project. Make sure it can be used as a folder name\n",
        "project_path = \"Loras/My_first_lora\" # @param {type: \"string\"}\n",
        "# @markdown Specify the name for the directories. If you have multiple datasets, separate each with a comma `(,)` like this: **dataset1, dataset2, ...**\n",
        "\n",
        "# @markdown The directory where the results of the training will be stored.\n",
        "output_dir_name = \"output\" # @param {type: \"string\"}\n",
        "# @markdown The directory where your dataset(s) will be located.\n",
        "dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n",
        "# @markdown Use Drive to store all the files and directories\n",
        "use_drive = True # @param {type: \"boolean\"}\n",
        "\n",
        "project_path = project_path.replace(\" \", \"_\")\n",
        "output_dir_name = output_dir_name.replace(\" \", \"_\")\n",
        "\n",
        "second_step_done = False\n",
        "\n",
        "def is_valid_folder_name(folder_name: str) -> bool:\n",
        "  invalid_characters = '<>:\"/\\|?*'\n",
        "\n",
        "  if any(char in invalid_characters for char in folder_name):\n",
        "    return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def mount_drive_dir() -> Path:\n",
        "  base_dir = root_path.joinpath(project_path)\n",
        "\n",
        "  if use_drive:\n",
        "    if not Path(drive_dir).exists():\n",
        "      drive.mount(Path(drive_dir).parent.as_posix())\n",
        "    base_dir = drive_dir.joinpath(project_path)\n",
        "\n",
        "  return base_dir\n",
        "\n",
        "def make_directories():\n",
        "  mount_drive = mount_drive_dir()\n",
        "  output_dir = mount_drive.joinpath(output_dir_name)\n",
        "\n",
        "  if not Path(mount_drive).exists():\n",
        "    Path(mount_drive).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  for dir in [pretrained_model_dir, vae_dir, output_dir, tagger_models_dir]:\n",
        "    Path(dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  for dataset_m_dir in dataset_dir_name.replace(\" \", \"\").split(','):\n",
        "    if is_valid_folder_name(dataset_m_dir):\n",
        "      Path(mount_drive.joinpath(dataset_m_dir)).mkdir(parents=True, exist_ok=True)\n",
        "    else:\n",
        "      print(f\"{dataset_m_dir} is not a valid name for a folder\")\n",
        "      return\n",
        "\n",
        "def main():\n",
        "  for name in [project_path, output_dir_name]:\n",
        "      if not is_valid_folder_name(name.replace(\"/\", \"\") if project_path == name else name):\n",
        "        print(f\"{name} is not a valid name for a folder\")\n",
        "        return\n",
        "\n",
        "  print(\"Setting up directories...\")\n",
        "  make_directories()\n",
        "  print(\"Done!\")\n",
        "\n",
        "try:\n",
        "  main()\n",
        "  second_step_done = True\n",
        "except Exception as e:\n",
        "  print(f\"Error setting up the directories!\\n{e}\")\n",
        "  second_step_done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b0_HNDa7Zdei",
        "cellView": "form",
        "outputId": "76c87ffb-559a-4c83-b25f-59399505cc5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model from https://huggingface.co/AstraliteHeart/pony-diffusion-v6/resolve/main/v6.safetensors...\n",
            " *** Download Progress Summary as of Sun Jun 29 22:18:10 2025 *** \n",
            "=\n",
            "[#52ff4e 1.2GiB/6.4GiB(18%) CN:16 DL:57MiB ETA:1m34s]\n",
            "FILE: //content/drive/MyDrive/Downloaded_models/v6.safetensors\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sun Jun 29 22:19:10 2025 *** \n",
            "=\n",
            "[#52ff4e 4.1GiB/6.4GiB(64%) CN:16 DL:42MiB ETA:55s]\n",
            "FILE: //content/drive/MyDrive/Downloaded_models/v6.safetensors\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "52ff4e|\u001b[1;32mOK\u001b[0m  |    48MiB/s|//content/drive/MyDrive/Downloaded_models/v6.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Downloading vae from https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors...\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "42bbf9|\u001b[1;32mOK\u001b[0m  |    65MiB/s|//content/drive/MyDrive/Downloaded_VAEs/sdxl_vae.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "# @title ## 3. Download the base model and/or VAE used for training ![doro fubuki](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_fubuki.png)\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "model_url = \"\"\n",
        "vae_url = \"\"\n",
        "\n",
        "# @markdown Default models are provided here for training. If you want to use another one, introduce the URL in the input below. The link must be pointing to either Civitai or Hugging Face and have the correct format. You can check how to get the correct link [here](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae).\n",
        "training_model = \"(XL) PonyDiffusion v6\" # @param [\"(XL) PonyDiffusion v6\", \"(XL) NoobAI Epsilon v1.0\", \"(XL) Illustrious v0.1\", \"(XL) Animagine 3.1\", \"(XL) SDXL 1.0\", \"(1.5) anime-full-final-pruned (Most used on Anime LoRAs)\", \"(1.5) AnyLora\", \"(1.5) SD 1.5\"]\n",
        "custom_training_model = \"\" # @param {type: \"string\"}\n",
        "# @markdown The name you want to give to the downloaded model file, if not specified default ones will be used.\n",
        "model_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown VAE used for training. It's not needed for 1.5 nor XL, but it's recommended to use the SDXL base VAE for XL training. If you want to use a custom one, introduce the URL in the input below.\n",
        "vae = \"SDXL VAE\" # @param [\"SDXL VAE\", \"None\"]\n",
        "custom_vae = \"\" # @param {type: \"string\"}\n",
        "# @markdown The name you want to give to the downloaded VAE file, if not specified default ones will be used.\n",
        "vae_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown Introduce your [Civitai API Token](https://civitai.com/user/account) or [HuggingFace Access Token](https://huggingface.co/settings/tokens) if the authentication fails while downloading the model and/or VAE.\n",
        "api_token = \"\" # @param {type: \"string\"}\n",
        "# @markdown You can optionally download the model and/or VAE on your drive so you don't need to download them again in the next session. You only would need to specify their path on the UI for the next time you want to use them.\n",
        "download_in_drive = True # @param {type: \"boolean\"}\n",
        "\n",
        "thrid_step_done = False\n",
        "\n",
        "if custom_training_model:\n",
        "  model_url = custom_training_model\n",
        "elif \"Pony\" in training_model:\n",
        "  model_url = \"https://huggingface.co/AstraliteHeart/pony-diffusion-v6/resolve/main/v6.safetensors\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.1/resolve/main/animagine-xl-3.1.safetensors\"\n",
        "elif \"SDXL\" in training_model:\n",
        "  model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
        "elif \"anime\" in training_model:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
        "elif \"Any\" in training_model:\n",
        "  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.safetensors\"\n",
        "elif \"SD 1.5\" in training_model:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "elif \"Illustrious\" in training_model:\n",
        "  model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0/resolve/main/Illustrious-XL-v0.1.safetensors\"\n",
        "elif \"NoobAI\" in training_model:\n",
        "  model_url = \"https://huggingface.co/Laxhar/noobai-XL-1.0/resolve/main/NoobAI-XL-v1.0.safetensors\"\n",
        "\n",
        "if custom_vae:\n",
        "  vae_url = custom_vae\n",
        "elif \"SDXL\" in vae:\n",
        "  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n",
        "\n",
        "model_file = \"\"\n",
        "vae_file = \"\"\n",
        "\n",
        "header = \"\"\n",
        "\n",
        "if not \"installed_dependencies\" in globals():\n",
        "  print(\"Installing missing dependency...\")\n",
        "  !apt -y update -qq\n",
        "  !apt install -y aria2 -qq\n",
        "  globals().setdefault(\"installed_dependencies\", True)\n",
        "\n",
        "def download_model():\n",
        "  global model_file, model_url, pretrained_model_dir\n",
        "\n",
        "  if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url):\n",
        "    model_url = model_url.replace(\"blob\", \"resolve\")\n",
        "  elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", model_url):\n",
        "    if m := re.search(r\"modelVersionId=(\\d+)\", model_url):\n",
        "      model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "  elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", model_url):\n",
        "    print(\"Invalid model download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n",
        "    return\n",
        "\n",
        "  if \"civitai.com\" in model_url and api_token and not \"hf\" in api_token:\n",
        "    model_url = f\"{model_url}&token={api_token}\" if \"?\" in model_url else f\"{model_url}?token={api_token}\"\n",
        "  elif \"huggingface.co\" in model_url and api_token:\n",
        "    header = f\"Authorization: Bearer {api_token}\"\n",
        "\n",
        "  stripped_model_url = model_url.strip()\n",
        "\n",
        "  if download_in_drive:\n",
        "    pretrained_model_dir = Path(drive_dir).joinpath(\"Downloaded_models\")\n",
        "\n",
        "    if not Path(pretrained_model_dir).exists():\n",
        "      Path(pretrained_model_dir).mkdir(exist_ok=True)\n",
        "\n",
        "  if model_name:\n",
        "    validated_name = model_name.translate(str.maketrans('', '', '\\\\/:*?\"<>|'))\n",
        "\n",
        "    if not validated_name.endswith((\".ckpt\", \".safetensors\")):\n",
        "      model_file = pretrained_model_dir.joinpath(f\"{validated_name}.safetensors\")\n",
        "    else:\n",
        "      model_file = pretrained_model_dir.joinpath(validated_name)\n",
        "  elif stripped_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = pretrained_model_dir.joinpath(stripped_model_url[stripped_model_url.rfind('/'):].replace(\"/\", \"\"))\n",
        "  else:\n",
        "    model_file = pretrained_model_dir.joinpath(\"downloaded_model.safetensors\")\n",
        "    if Path(model_file).exists() and not download_in_drive:\n",
        "      !rm \"{model_file}\"\n",
        "\n",
        "  print(f\"Downloading model from {model_url}...\")\n",
        "  !aria2c \"{model_url}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "def download_vae():\n",
        "  global vae_file, vae_url, vae_dir\n",
        "\n",
        "  if not vae == \"None\":\n",
        "    if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url):\n",
        "      vae_url = vae_url.replace(\"blob\", \"resolve\")\n",
        "    elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", vae_url):\n",
        "      if m := re.search(r\"modelVersionId=(\\d+)\", vae_url):\n",
        "        vae_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "    elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", vae_url):\n",
        "      print(\"Invalid VAE download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n",
        "      return\n",
        "\n",
        "    if \"civitai.com\" in vae_url and api_token and not \"hf\" in api_token:\n",
        "      vae_url = f\"{vae_url}&token={api_token}\" if \"?\" in vae_url else f\"{vae_url}?token={api_token}\"\n",
        "    elif \"huggingface.co\" in vae_url and api_token:\n",
        "      header = f\"Authorization: Bearer {api_token}\"\n",
        "\n",
        "    stripped_model_vae = vae_url.strip()\n",
        "\n",
        "    if download_in_drive:\n",
        "      vae_dir = Path(drive_dir).joinpath(\"Downloaded_VAEs\")\n",
        "\n",
        "      if not Path(vae_dir).exists():\n",
        "        Path(vae_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    if vae_name:\n",
        "      validated_name = vae_name.translate(str.maketrans('', '', '\\\\/:*?\"<>|'))\n",
        "\n",
        "      if not validated_name.endswith((\".ckpt\", \".safetensors\")):\n",
        "        vae_file = vae_dir.joinpath(f\"{validated_name}.safetensors\")\n",
        "      else:\n",
        "        vae_file = vae_dir.joinpath(validated_name)\n",
        "    elif stripped_model_vae.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "      vae_file = vae_dir.joinpath(stripped_model_vae[stripped_model_vae.rfind('/'):].replace(\"/\", \"\"))\n",
        "    else:\n",
        "      vae_file = vae_dir.joinpath(\"downloaded_vae.safetensors\")\n",
        "      if Path(vae_file).exists() and not download_in_drive:\n",
        "        !rm \"{vae_file}\"\n",
        "\n",
        "    print(f\"Downloading vae from {vae_url}...\")\n",
        "    !aria2c \"{vae_url}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n",
        "  else:\n",
        "    vae_file = \"\"\n",
        "\n",
        "def main():\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You have to run the 2nd step first!\")\n",
        "    return\n",
        "\n",
        "  if download_in_drive and not use_drive:\n",
        "    print(\"You are trying to download the model and/or VAE in your drive but you didn't mount it. Please select the 'use_drive' option in 2nd step.\")\n",
        "    return\n",
        "\n",
        "  download_model()\n",
        "  download_vae()\n",
        "\n",
        "try:\n",
        "  main()\n",
        "  thrid_step_done = True\n",
        "except Exception as e:\n",
        "  print(f\"Failed to download the models\\n{e}\")\n",
        "  thrid_step_done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "66XBK6B_iSYj",
        "outputId": "4c97d182-1809-4f32-d5c7-6f81c36fc9c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Dataset extracted in /content/drive/MyDrive/Loras/My_first_lora/dataset\n"
          ]
        }
      ],
      "source": [
        "# @title ## 4. Upload your dataset ![doro shifty](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_shifty.png)\n",
        "import re\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown ### Unzip the dataset\n",
        "# @markdown If you have a dataset in a zip file, you can specify the path to it below. This will extract the dataset into the dataset directory specified in step 2. It supports downloading the zip from **HuggingFace**. To get the correct link you only need to follow the steps [for models/VAEs](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#from-huggingface) but applying them to the zip file.\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/dataset.zip\" # @param {type: \"string\"}\n",
        "# @markdown Specify the name of your dataset directory. If it doesn't exist, it will be created. If you have multiple dataset directories, extract each zip file into its respective dataset directory.\n",
        "extract_to_dataset_dir = \"dataset\" # @param {type: \"string\"}\n",
        "# @markdown Provide a [HuggingFace Access Token](https://huggingface.co/settings/tokens) if your dataset is in a private repository.\n",
        "hf_token = \"\" # @param {type: \"string\"}\n",
        "\n",
        "if not \"installed_dependencies\" in globals():\n",
        "  print(\"Installing missing dependency...\")\n",
        "  !apt -y update -qq\n",
        "  !apt install -y aria2 -qq\n",
        "  globals().setdefault(\"installed_dependencies\", True)\n",
        "\n",
        "def extract_dataset():\n",
        "  global zip_path\n",
        "  is_from_hf = False\n",
        "\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  if zip_path.startswith(\"https://huggingface.co/\"):\n",
        "    is_from_hf = True\n",
        "\n",
        "  if not Path(zip_path).exists() and not is_from_hf:\n",
        "    print(\"The path of the zip doesn't exists!\")\n",
        "    return\n",
        "\n",
        "  if \"drive/MyDrive\" in zip_path and not Path(drive_dir).exists():\n",
        "    print(\"Your trying to access drive but you didn't mount it!\")\n",
        "    return\n",
        "\n",
        "  dataset_dir = root_path.joinpath(project_path, extract_to_dataset_dir)\n",
        "  if Path(drive_dir).exists():\n",
        "    dataset_dir = drive_dir.joinpath(project_path, extract_to_dataset_dir)\n",
        "\n",
        "  if not Path(dataset_dir).exists():\n",
        "    Path(dataset_dir).mkdir(exist_ok=True)\n",
        "    print(f\"Created dataset directory on new location because it didn't exist before: {dataset_dir}\")\n",
        "\n",
        "  if is_from_hf and re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n",
        "    print(\"Zip file from HuggingFace detected, attempting to download...\")\n",
        "\n",
        "    if \"blob\" in zip_path:\n",
        "      zip_path = zip_path.replace(\"blob\", \"resolve\")\n",
        "    header = f\"Authorization: Bearer {hf_token}\" if hf_token else \"\"\n",
        "\n",
        "    !aria2c \"{zip_path}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"/content/dataset.zip\"\n",
        "    zip_path = \"/content/dataset.zip\"\n",
        "  elif is_from_hf and not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n",
        "    print(\"Invalid URL provided for downloading the zip file.\")\n",
        "    return\n",
        "\n",
        "  print(\"Extracting dataset...\")\n",
        "\n",
        "  with zipfile.ZipFile(zip_path, 'r') as f:\n",
        "    f.extractall(dataset_dir)\n",
        "\n",
        "  print(f\"Dataset extracted in {dataset_dir}\")\n",
        "\n",
        "  if is_from_hf:\n",
        "    print(\"Removing temporary zip file...\")\n",
        "    !rm \"{zip_path}\"\n",
        "    print(\"Done!\")\n",
        "\n",
        "extract_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "J86M4s3ohUYv",
        "outputId": "1d361db4-38a3-4e9a-8bc2-95a866e4a342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing missing dependencies...\n",
            "Collecting fairscale==0.4.13\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.6.12\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from fairscale==0.4.13) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.22.0 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from fairscale==0.4.13) (1.26.4)\n",
            "Requirement already satisfied: pyyaml in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from timm==0.6.12) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from timm==0.6.12) (0.24.5)\n",
            "Requirement already satisfied: torchvision in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from timm==0.6.12) (0.21.0+cu124)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (10.3.5.147)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (1.13.1)\n",
            "Requirement already satisfied: filelock in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (3.13.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (2.21.5)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (11.2.1.3)\n",
            "Requirement already satisfied: jinja2 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.4.127)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (4.12.2)\n",
            "Requirement already satisfied: fsspec in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.3.1.170)\n",
            "Requirement already satisfied: networkx in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (3.3)\n",
            "Requirement already satisfied: triton==3.2.0 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (3.2.0)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (0.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.8.0->fairscale==0.4.13) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from huggingface-hub->timm==0.6.12) (25.0)\n",
            "Requirement already satisfied: requests in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from huggingface-hub->timm==0.6.12) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from huggingface-hub->timm==0.6.12) (4.67.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from torchvision->timm==0.6.12) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->fairscale==0.4.13) (2.1.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.6.12) (2025.6.15)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.6.12) (2.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.6.12) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.6.12) (3.10)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332208 sha256=1cb0536efbef17dbda92257e20324739d759af1e11724a6d19219e366627c308\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale, timm\n",
            "Successfully installed fairscale-0.4.13 timm-0.6.12\n",
            "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
            "Collecting onnxruntime-gpu==1.20.1\n",
            "  Downloading onnxruntime_gpu-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (291.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.5/291.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from onnxruntime-gpu==1.20.1) (1.26.4)\n",
            "Collecting flatbuffers\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: packaging in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from onnxruntime-gpu==1.20.1) (25.0)\n",
            "Collecting coloredlogs\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/coloredlogs/15.0.1/coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from onnxruntime-gpu==1.20.1) (1.13.1)\n",
            "Requirement already satisfied: protobuf in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from onnxruntime-gpu==1.20.1) (6.31.1)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/humanfriendly/10/humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./trainer/sd_scripts/venv/lib/python3.10/site-packages (from sympy->onnxruntime-gpu==1.20.1) (1.3.0)\n",
            "Installing collected packages: flatbuffers, humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 flatbuffers-25.2.10 humanfriendly-10.0 onnxruntime-gpu-1.20.1\n",
            "Tagging images\n",
            "rich is not installed, using basic logging\n",
            "get_preferred_device() -> cuda\n",
            "Current Working Directory is: /content/trainer/sd_scripts\n",
            "load images from /content/drive/MyDrive/Loras/My_first_lora/dataset\n",
            "found 17 images.\n",
            "loading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 272kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 3.26MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 6.71MB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 3.55MB/s]\n",
            "Downloading: \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\" to /root/.cache/torch/hub/checkpoints/model_large_caption.pth\n",
            "\n",
            "100% 1.66G/1.66G [00:07<00:00, 240MB/s]\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
            "BLIP loaded\n",
            "100% 3/3 [00:10<00:00,  3.64s/it]\n",
            "done!\n",
            "Tagging complete!\n"
          ]
        }
      ],
      "source": [
        "# @markdown ### Tag your images ![doro syuen](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_syuen.png)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown As the name suggests, this is the type of tagging you want for your dataset.\n",
        "method = \"Photorealistic\" # @param [\"Anime\", \"Photorealistic\"]\n",
        "# @markdown `(Only applies to Anime method)` The default model used for tagging is `SmilingWolf/wd-eva02-large-tagger-v3`. I find it more accurate than other taggers, but if you have experience, you can use another one and tweak the parameters. If you don't, the default configuration should be fine.\n",
        "model = \"SmilingWolf/wd-eva02-large-tagger-v3\" # @param [\"SmilingWolf/wd-eva02-large-tagger-v3\", \"SmilingWolf/wd-vit-large-tagger-v3\", \"SmilingWolf/wd-swinv2-tagger-v3\", \"SmilingWolf/wd-vit-tagger-v3\", \"SmilingWolf/wd-convnext-tagger-v3\", \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"SmilingWolf/wd-v1-4-moat-tagger-v2\", \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\", \"SmilingWolf/wd-v1-4-convnext-tagger-v2\", \"SmilingWolf/wd-v1-4-vit-tagger-v2\"]\n",
        "# @markdown The directory name of the dataset you want to tag. You can specify another directory when the previous one is fully tagged, in case you have more than one dataset.\n",
        "dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n",
        "# @markdown The type of file to save your captions.\n",
        "file_extension = \".txt\" # @param [\".txt\", \".caption\"]\n",
        "# @markdown `(Only applies to Anime method)` Specify the tags that you don't want the autotagger to use. Separate each one with a comma `(,)` like this: **1girl, solo, standing, ...**\n",
        "blacklisted_tags = \"\" # @param {type: \"string\"}\n",
        "# @markdown `(Only applies to Anime method)` Specify the minimum confidence level required for assigning a tag to the image. A lower threshold results in more tags being assigned. The recommended default value for v2 taggers is 0.35 and for v3 is 0.25.\n",
        "threshold = 0.25 # @param {type: \"slider\", min:0.0, max: 1.0, step:0.01}\n",
        "# @markdown `(Only applies to Photorealistic method)` Specify the minimum number of words (also known as tokens) to include in the captions.\n",
        "caption_min = 10 # @param {type: \"number\"}\n",
        "# @markdown `(Only applies to Photorealistic method)` Specify the maximum number of words (also known as tokens) to include in the captions.\n",
        "caption_max = 75 # @param {type: \"number\"}\n",
        "\n",
        "blacklisted_tags = blacklisted_tags.replace(\" \", \"\")\n",
        "\n",
        "def caption_images():\n",
        "  global use_onnx_runtime\n",
        "\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  dataset_dir = root_path.joinpath(project_path, dataset_dir_name)\n",
        "  if Path(drive_dir).exists():\n",
        "    dataset_dir = drive_dir.joinpath(project_path, dataset_dir_name)\n",
        "\n",
        "  sd_scripts = trainer_dir.joinpath(\"sd_scripts\")\n",
        "  if not globals().get(\"first_step_done\"):\n",
        "    print(\"Please run the step 1 first.\")\n",
        "    return\n",
        "\n",
        "  if not globals().get(\"tagger_dependencies\"):\n",
        "    print(\"Installing missing dependencies...\")\n",
        "    !{venv_pip} install fairscale==0.4.13 timm==0.6.12\n",
        "    !{venv_pip} install onnxruntime-gpu==1.20.1 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
        "    globals().setdefault(\"tagger_dependencies\", True)\n",
        "\n",
        "  batch_size = 8 if \"v3\" in model or \"swinv2\" in model else 1\n",
        "\n",
        "  model_dir = tagger_models_dir.joinpath(model.split(\"/\")[-1])\n",
        "\n",
        "  print(\"Tagging images\")\n",
        "\n",
        "  if method == \"Anime\":\n",
        "    !{venv_python} {wd_path} \\\n",
        "      {dataset_dir} \\\n",
        "      --repo_id={model} \\\n",
        "      --model_dir={model_dir} \\\n",
        "      --thresh={threshold} \\\n",
        "      --batch_size={batch_size} \\\n",
        "      --max_data_loader_n_workers=2 \\\n",
        "      --caption_extension={file_extension} \\\n",
        "      --undesired_tags={blacklisted_tags} \\\n",
        "      --remove_underscore \\\n",
        "      --onnx\n",
        "  else:\n",
        "    os.chdir(sd_scripts)\n",
        "    !{venv_python} finetune/make_captions.py \\\n",
        "      {dataset_dir} \\\n",
        "      --beam_search \\\n",
        "      --max_data_loader_n_workers=2 \\\n",
        "      --batch_size=8 \\\n",
        "      --min_length={caption_min} \\\n",
        "      --max_length={caption_max} \\\n",
        "      --caption_extension=.txt\n",
        "    os.chdir(root_path)\n",
        "\n",
        "  print(\"Tagging complete!\")\n",
        "\n",
        "caption_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "PC5JsouHTr26",
        "outputId": "e86806ab-bfba-4565-a297-f238d421c90b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset paths:\n",
            "  Dataset directory 1: /content/drive/MyDrive/Loras/My_first_lora/dataset\n",
            "Model path: /content/drive/MyDrive/Downloaded_models/v6.safetensors\n",
            "VAE path: /content/drive/MyDrive/Downloaded_VAEs/sdxl_vae.safetensors\n",
            "Output path: /content/drive/MyDrive/Loras/My_first_lora/output\n",
            "Config file path: It's saved locally on your machine\n",
            "Tags file path: It's saved locally on your machine\n"
          ]
        }
      ],
      "source": [
        "# @title ## 5. Start the training ![doro cinderella](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_cinderella.png)\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown Execute this cell to obtain the paths. Input these paths into the UI to start the training.\n",
        "\n",
        "def print_paths():\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  dataset_dirs = []\n",
        "  project_base_dir = root_path.joinpath(project_path)\n",
        "  if globals().get(\"use_drive\"):\n",
        "    project_base_dir = drive_dir.joinpath(project_path)\n",
        "\n",
        "  for id, p_dataset_m_dir in enumerate(dataset_dir_name.replace(\" \", \"\").split(',')):\n",
        "    dataset_dirs.append(f\"Dataset directory {id + 1}: {project_base_dir.joinpath(p_dataset_m_dir)}\")\n",
        "\n",
        "  model_path = model_file or \"None or you didn't run the cell to download it either because you forgot or because you have the model in drive\"\n",
        "  vae_path = vae_file or \"None or you didn't run the cell to download it either because you forgot or because you have the VAE in drive\"\n",
        "  output_path = project_base_dir.joinpath(output_dir_name)\n",
        "\n",
        "  print(\"Dataset paths:\\n  {0}\\nModel path: {1}\\nVAE path: {2}\\nOutput path: {3}\\nConfig file path: {4}\\nTags file path: {4}\".format('\\n  '.join(dataset_dirs), model_path.as_posix().replace(\" \", \"\"), vae_path, output_path, \"It's saved locally on your machine\"))\n",
        "\n",
        "print_paths()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "80gDArpjBLk-",
        "outputId": "2d05aa28-b708-4cea-8ff8-47c25fa0688c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rich is not installed, using basic logging\n",
            "Starting server...\n",
            " * Running on https://favors-favourites-dh-eagles.trycloudflare.com\n",
            " * Traffic stats available on http://127.0.0.1:20241/metrics\n",
            "Server started\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown Running this cell will create a tunnel that allows you to connect from your local UI so you can send the training settings to colab. If you don't have it installed, please install it [from here](https://github.com/derrian-distro/LoRA_Easy_Training_Scripts). Read the [instructions for installation](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-install-the-ui). Once you launch the UI, set up your training parameters, copy the given URL into your interface, and click \"Start training\".\n",
        "\n",
        "# @markdown `(Optional)` Ngrok is an alternative method, and you need a token that you can obtain from [Ngrok's dashboard](https://dashboard.ngrok.com/get-started/your-authtoken). I recommend using it only if you want, have experience, or if the default tunnel provider is down. [How to obtain Ngrok token](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-obtain-the-ngrok-token)\n",
        "\n",
        "use_ngrok = False # @param {type: \"boolean\"}\n",
        "ngrok_token = \"\" # @param {type: \"string\"}\n",
        "\n",
        "fifth_step_done = False\n",
        "\n",
        "def init_tunnel():\n",
        "  global fifth_step_done\n",
        "\n",
        "  if not globals().get(\"first_step_done\"):\n",
        "    print(\"Please run the 1st step first.\")\n",
        "    return\n",
        "\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  config_file = trainer_dir.joinpath(\"config.json\")\n",
        "\n",
        "  if use_ngrok:\n",
        "    if not ngrok_token:\n",
        "      print(\"The ngrok token must not be empty!\")\n",
        "      return\n",
        "\n",
        "    with open(config_file, 'r') as config:\n",
        "      data = json.load(config)\n",
        "\n",
        "    data[\"remote_mode\"] = \"ngrok\"\n",
        "    data[\"ngrok_token\"] = ngrok_token\n",
        "\n",
        "    with open(config_file, 'w') as config:\n",
        "      json.dump(data, config, indent=2)\n",
        "  else:\n",
        "    with open(config_file, 'r') as config:\n",
        "      data = json.load(config)\n",
        "\n",
        "    if data[\"remote_mode\"] == \"ngrok\":\n",
        "      data[\"remote_mode\"] = \"cloudflared\"\n",
        "      data[\"ngrok_token\"] = \"\"\n",
        "\n",
        "      with open(config_file, 'w') as config:\n",
        "        json.dump(data, config, indent=2)\n",
        "\n",
        "  os.chdir(trainer_dir)\n",
        "  !chmod 755 run.sh\n",
        "  !./run.sh\n",
        "  os.chdir(root_path)\n",
        "\n",
        "  fifth_step_done = True\n",
        "\n",
        "init_tunnel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ufU4_DUl2Rzv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown Run this cell to start the training\n",
        "\n",
        "# @markdown Are you training on sdxl?\n",
        "sdxl = True # @param {type: \"boolean\"}\n",
        "\n",
        "def start_training(is_sdxl: bool):\n",
        "  if not globals().get(\"fifth_step_done\"):\n",
        "    print(\"Run the cell above this one first!\")\n",
        "    return\n",
        "\n",
        "  os.chdir(trainer_dir)\n",
        "\n",
        "  config = Path(\"runtime_store/config.toml\").resolve()\n",
        "  dataset = Path(\"runtime_store/dataset.toml\").resolve()\n",
        "\n",
        "  if not Path(config).exists() and not Path(dataset).exists():\n",
        "    print(\"The required files were not generated while running the above cell, please check again!\")\n",
        "    return\n",
        "\n",
        "  sd_scripts = Path(\"sd_scripts\").resolve()\n",
        "  training_network = \"sdxl_train_network.py\" if is_sdxl else \"train_network.py\"\n",
        "\n",
        "  !{venv_python} {sd_scripts.joinpath(training_network)} \\\n",
        "    --config_file={config} \\\n",
        "    --dataset_config={dataset}\n",
        "\n",
        "  os.chdir(root_path)\n",
        "\n",
        "start_training(sdxl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Utils ![doro anachiro](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_anachiro.png)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown ### LoRA Resizer ![doro grave](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_grave.png)\n",
        "\n",
        "# @markdown The path pointing to the LoRA file you want to resize.\n",
        "lora = \"\" # @param {type: \"string\"}\n",
        "# @markdown `(Optional)` The path of the directory where the resized LoRA will be saved. If not specified the parent directory of the loaded LoRA will be used.\n",
        "output_dir = \"\" # @param {type: \"string\"}\n",
        "# @markdown `(Optional)` The name for the resized LoRA file. If not specified the name of the loaded LoRA will be used appending **_resized** to it.\n",
        "output_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown The precision for saving the resized LoRA. `fp16` is the usual precision to use. **Don't touch unless you know what you are doing!**\n",
        "save_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"float\"]\n",
        "# @markdown The new dimensions, aka dim, for the LoRA.\n",
        "new_dim = 4 # @param {type: \"number\"}\n",
        "# @markdown `(LoCon-like networks only)` The new conv dimensions, aka conv dim, for the LoRA. Only use on networks that are trained with conv. For example: **LoCon, LyCORIS, LoHa, Lokr, etc**. Keep the value less than 1 to omit it's usage.\n",
        "new_conv_dim = 0 # @param {type: \"number\"}\n",
        "# @markdown Enables/disables the usage of `dynamic_method` and `dynamic_param`. **Don't touch unless you know what you are doing!**\n",
        "use_dynamic = False # @param {type: \"boolean\"}\n",
        "# @markdown Method used to calculate the resize. `sv_fro` is the usual method to use.\n",
        "dynamic_method = \"sv_fro\" # @param [\"sv_fro\", \"sv_ratio\", \"sv_cumulative\"]\n",
        "# @markdown Value used by the `dynamic_method` to calculate the resize.\n",
        "dynamic_param = 0.9700 # @param {type: \"number\"}\n",
        "# @markdown Use the GPU resources to resize the LoRA. If disabled it will use the CPU which is **not recommended!**\n",
        "use_gpu = True # @param {type: \"boolean\"}\n",
        "# @markdown Prints in the console the information about the resizing when the process finishes.\n",
        "verbose_printing = False # @param {type: \"boolean\"}\n",
        "# @markdown `(LoCon-like networks only)` Removes the conv dim layers from the LoRA. Only use on networks that are trained with conv. For example: **LoCon, LyCORIS, LoHa, Lokr, etc. Don't touch unless you know what you are doing!**\n",
        "remove_conv_dims = False # @param {type: \"boolean\"}\n",
        "# @markdown Removes the linear dim layers (which is what is trained usually in a LoRA) from the LoRA. **Don't touch unless you know what you are doing!**\n",
        "remove_linear_dims = False # @param {type: \"boolean\"}\n",
        "\n",
        "def validate() -> tuple[bool, bool]:\n",
        "  global output_dir, output_name\n",
        "\n",
        "  failed = False\n",
        "  use_conv = True\n",
        "  if not globals().get(\"first_step_done\"):\n",
        "    print(\"Please run the 1st step first.\")\n",
        "    failed = True\n",
        "\n",
        "  if not Path(lora).is_file() or Path(lora).suffix not in [\".ckpt\", \".safetensors\"]:\n",
        "    print(\"The path to the LoRA file is invalid.\")\n",
        "    failed = True\n",
        "\n",
        "  if not Path(output_dir).is_dir() or not output_dir:\n",
        "    output_dir = Path(output_dir).parent if output_dir else Path(lora).parent\n",
        "    if not output_dir.is_dir():\n",
        "      print(\"The path to the output folder is invalid, or not a folder\")\n",
        "      failed = True\n",
        "    output_dir = output_dir.as_posix()\n",
        "\n",
        "  if not output_name:\n",
        "    output_name = f\"{Path(lora).name.split('.')[0]}_resized\"\n",
        "  else:\n",
        "    output_name = output_name.split(\".\")[0]\n",
        "\n",
        "  if Path(output_dir).joinpath(f\"{output_name}.safetensors\").exists():\n",
        "    idx = 1\n",
        "    temp_name = output_name\n",
        "    while Path(output_dir).joinpath(f\"{output_name}.safetensors\").exists():\n",
        "      output_name = f\"{temp_name}_{idx}\"\n",
        "      idx += 1\n",
        "\n",
        "    print(f\"Duplicated file in the output directory, file name changed to {output_name}\")\n",
        "\n",
        "  if new_dim < 1:\n",
        "    print(\"The new dim must be 1 or greater\")\n",
        "    failed = True\n",
        "\n",
        "  if new_conv_dim < 1:\n",
        "    print(\"Skipping setting new conv dim, using new dim only\")\n",
        "    use_conv = False\n",
        "\n",
        "  if use_dynamic and dynamic_param <= 0:\n",
        "    print(\"The dynamic param must be greater than 0\")\n",
        "    failed = True\n",
        "\n",
        "  return failed, use_conv\n",
        "\n",
        "def resize_lora(use_conv: bool):\n",
        "  output_file = Path(output_dir).joinpath(f\"{output_name}.safetensors\").resolve()\n",
        "\n",
        "  new_conv_arg = f\"--new_conv_rank={new_conv_dim}\" if use_conv else \"\"\n",
        "  dynamic_method_arg = f\"--dynamic_method={dynamic_method}\" if use_dynamic else \"\"\n",
        "  dynamic_param_arg = \"--dynamic_param={0:.4f}\".format(dynamic_param) if use_dynamic else \"\"\n",
        "\n",
        "  os.chdir(trainer_dir)\n",
        "\n",
        "  !{venv_python} {Path(\"utils/resize_lora.py\").resolve()} \\\n",
        "    --model={lora} \\\n",
        "    --save_precision={save_precision} \\\n",
        "    --new_rank={new_dim} \\\n",
        "    --save_to={output_file} \\\n",
        "    {new_conv_arg} \\\n",
        "    {dynamic_method_arg} \\\n",
        "    {dynamic_param_arg} \\\n",
        "    {\"--verbose\" if verbose_printing else \"\"} \\\n",
        "    {\"--device=cuda\" if use_gpu else \"\"} \\\n",
        "    {\"--del_conv\" if remove_conv_dims else \"\"} \\\n",
        "    {\"--del_linear\" if remove_linear_dims else \"\"} \\\n",
        "\n",
        "  os.chdir(root_path)\n",
        "\n",
        "def main():\n",
        "  failed, use_conv = validate()\n",
        "  if failed:\n",
        "    return\n",
        "\n",
        "  resize_lora(use_conv)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "pEf-buIXyDLg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}